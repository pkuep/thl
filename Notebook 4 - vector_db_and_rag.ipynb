{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verwendete Bibliotheken & Initialisierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Falls nötig:\n",
    "# pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "# pip install -U \"transformers[torch]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters  import RecursiveCharacterTextSplitter\n",
    "# from langchain_text_splitters import TokenTextSplitter\n",
    "# Alternative, wenn anstelle von Zeichen anhand von Token gesplittet werden soll \n",
    "# --> Realistischere Token-Grenzen durch Splitting möglich\n",
    "# splitter = TokenTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from termcolor import colored\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vektordatenbank aus Bedienungsanleitungen erzeugen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector DB created and saved to ./vector_db_manuals\n"
     ]
    }
   ],
   "source": [
    "# Load the manuals\n",
    "docs = []\n",
    "manuals_path = 'manuals/'\n",
    "for path in [f\"{manuals_path}stellarwave_a9_manual.md\", f\"{manuals_path}luminor_arc65_manual.md\"]:\n",
    "    loader = TextLoader(path, encoding='utf-8')\n",
    "    docs.extend(loader.load())\n",
    "\n",
    "# Dokumente in einzelne Abschnitte unterteilen\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "docs_split = splitter.split_documents(docs)\n",
    "\n",
    "# Embeddings erzeugen, spezieller Embedding-Encoder (ACHTUNG: OpenAI-API)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Speicherung in einer FAISS Vektordatenbank\n",
    "vectorstore = FAISS.from_documents(docs_split, embeddings)\n",
    "vectorstore.save_local(\"vector_db_manuals\")\n",
    "\n",
    "print(\"Vector DB created and saved to ./vector_db_manuals\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beispielhafte Abfrage der Vektordatenbank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mAnfrage: How can I install my TV?\u001b[0m\n",
      "\u001b[32mTreffer 1: (Score: 1.07)\u001b[0m\n",
      "\u001b[32m------------------------\u001b[0m\n",
      "### 3.2 Mounting Instructions\n",
      "1. Remove the stand before wall installation.  \n",
      "2. Align holes with **VESA 400 × 300 mm** bracket.  \n",
      "3. Use **M6 × 16 mm** screws.  \n",
      "4. Route cables through the provided rear channels for clean setup.\n",
      "\n",
      "### 3.3 Placement Tips\n",
      "- For fireplace setups: use a tilting bracket to maintain viewing angle below 15°.  \n",
      "- For best color uniformity, view from at least **1.2× screen height** distance.\n",
      "--> (Quelle: manuals/luminor_arc65_manual.md)\n",
      "\n",
      "\u001b[32mTreffer 2: (Score: 1.30)\u001b[0m\n",
      "\u001b[32m------------------------\u001b[0m\n",
      "### 3.2 Mounting\n",
      "The A9 supports both **desk stand** and **VESA 100 × 100 mm** wall mounts.  \n",
      "For wall installation:\n",
      "1. Remove the stand using a Phillips screwdriver.  \n",
      "2. Align the bracket holes.  \n",
      "3. Use **M4 × 10 mm** screws (not included).  \n",
      "4. Maintain at least **10 cm clearance** around the rear vents.\n",
      "--> (Quelle: manuals/stellarwave_a9_manual.md)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Beispielabfrage \n",
    "query = \"How can I install my TV?\"\n",
    "\n",
    "print(colored(f\"Anfrage: {query}\", \"cyan\", attrs=[\"bold\"]))\n",
    "\n",
    "# Suche in Vektordb\n",
    "results = vectorstore.similarity_search_with_score(query, k=2)  # k = Anzahl Treffer\n",
    "\n",
    "# Ausgabe der Treffer\n",
    "for i, (doc, score) in enumerate(results, start=1):\n",
    "    print(colored(f\"Treffer {i}: (Score: {score:.2f})\", \"green\"))\n",
    "    print(colored(\"------------------------\", \"green\"))\n",
    "    print(doc.page_content.strip())\n",
    "    if 'source' in doc.metadata:\n",
    "        print(f\"--> (Quelle: {doc.metadata['source']})\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Übergabe der Informationen an ein LLM mit RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36m---- PROMPT SENT TO LLM ----\u001b[0m\n",
      "\u001b[36mHuman: You are a helpful TV/monitor support assistant.\n",
      "Use ONLY the CONTEXT to answer. If it's not in the manuals, \n",
      "say so by answering 'NO IMMEDIATE SOLUTION POSSIBLE' only!\n",
      "\n",
      "QUESTION:\n",
      "How can I mount my TV?\n",
      "\n",
      "CONTEXT:\n",
      "[1] ### 3.2 Mounting Instructions\n",
      "1. Remove the stand before wall installation.  \n",
      "2. Align holes with **VESA 400 × 300 mm** bracket.  \n",
      "3. Use **M6 × 16 mm** screws.  \n",
      "4. Route cables through the provided rear channels for clean setup.\n",
      "\n",
      "### 3.3 Placement Tips\n",
      "- For fireplace setups: use a tilting bracket to maintain viewing angle below 15°.  \n",
      "- For best color uniformity, view from at least **1.2× screen height** distance.\n",
      "\n",
      "[2] ### 3.2 Mounting\n",
      "The A9 supports both **desk stand** and **VESA 100 × 100 mm** wall mounts.  \n",
      "For wall installation:\n",
      "1. Remove the stand using a Phillips screwdriver.  \n",
      "2. Align the bracket holes.  \n",
      "3. Use **M4 × 10 mm** screws (not included).  \n",
      "4. Maintain at least **10 cm clearance** around the rear vents.\n",
      "\n",
      "[3] ## 2. Safety & Handling\n",
      "- Avoid exposure to direct sunlight for extended periods.\n",
      "- Do not cover ventilation openings — the rear cooling vents must remain unobstructed.\n",
      "- When wall-mounting, ensure the bracket supports at least **10 kg**.\n",
      "- Disconnect the monitor from power before cleaning. Use a **microfiber cloth only**.\n",
      "\n",
      "Answer in English. If steps are needed, list them.\u001b[0m\n",
      "\u001b[1m\u001b[36m-----------------------------\u001b[0m\n",
      "\u001b[32mTo mount your TV, follow these steps based on the provided context:\n",
      "\n",
      "1. Remove the stand before wall installation.\n",
      "2. Align the holes with the appropriate VESA bracket (either **VESA 400 × 300 mm** or **VESA 100 × 100 mm**, depending on your TV model).\n",
      "3. Use the correct screws: **M6 × 16 mm** screws for the 400 × 300 mm configuration and **M4 × 10 mm** screws for the 100 × 100 mm configuration (note that these screws are not included).\n",
      "4. Ensure that the bracket supports at least **10 kg** (this is important for safety).\n",
      "5. Route cables through the provided rear channels for a clean setup.\n",
      "6. Maintain at least **10 cm clearance** around the rear vents to ensure proper ventilation.\n",
      "\n",
      "Make sure to disconnect the TV/monitor from power before beginning the installation process.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Minimaler RAG-Flow \n",
    "\n",
    "# Datenbankzugriff auf Vektordatenbank vorbereiten\n",
    "vectorstore = FAISS.load_local(\"vector_db_manuals\", embeddings, allow_dangerous_deserialization=True)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# Prompt-Vorlage\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are a helpful TV/monitor support assistant.\n",
    "Use ONLY the CONTEXT to answer. If it's not in the manuals, \n",
    "say so by answering 'NO IMMEDIATE SOLUTION POSSIBLE' only!\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "Answer in English. If steps are needed, list them.\"\"\"\n",
    ")\n",
    "\n",
    "# Erzeuge einen Ablauf (chain):\n",
    "# Prompt erzeugen --> LLM aufrufen --> Ausgabe als String\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "parser = StrOutputParser()\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "# Hilfsfunktion: Docs in einen String verwandeln\n",
    "def make_context(docs):\n",
    "    return \"\\n\\n\".join(f\"[{i+1}] {d.page_content.strip()}\" for i, d in enumerate(docs))\n",
    "\n",
    "# „ask“-Funktion\n",
    "def ask(question: str) -> str:\n",
    "    docs = retriever.invoke(question)\n",
    "    ctx = make_context(docs)\n",
    "    \n",
    "    full_prompt = prompt.format(question=question, context=ctx)\n",
    "    print(colored(\"---- PROMPT SENT TO LLM ----\", \"cyan\",  attrs=[\"bold\"]))\n",
    "    print(colored(full_prompt, \"cyan\"))\n",
    "    print(colored(\"-----------------------------\", \"cyan\", attrs=[\"bold\"]))\n",
    "    return chain.invoke({\"question\": question, \"context\": ctx})\n",
    "\n",
    "# Beispiel-Ticket (ACHTUNG: Nutzung der OpenAI-API! Kosten entstehen)\n",
    "# question = \"My StellarWave A9 screen flickers on DisplayPort. What can I do?\"\n",
    "# question = \"My StellarWave A9 is lying on the carpet. What can I do?\"\n",
    "question = \"How can I mount my TV?\"\n",
    "\n",
    "print(colored(ask(question), \"green\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation der Antwort (hybrid: Vektorsuche + LLM as a Judge)\n",
    "Ansatz: Sowohl die \"Nähe\" in der Vektordatenbank-Suche ermitteln, als auch die Antwort des LLMs durch ein weiteres LLM (LLM as a Judge) beurteilen lassen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mQuestion: How can I mount my TV?\u001b[0m\n",
      "\u001b[1m\u001b[32mConfidence (final): 0.71 (retrieval 0.51, judge model 1.00)\u001b[0m\n",
      "\u001b[33mRationale: The provided context outlines clear mounting instructions, specifying tool requirements and safety considerations.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def retrieve_from_vector_db_with_scores(query: str, k: int = 3):\n",
    "    \"\"\" Liefert die Nähe der ermittelten Dokumente zur Query. Je kleiner der Score desto besser. \"\"\"\n",
    "    docs_scores = vectorstore.similarity_search_with_score(query, k=k)  # List[ (Document, score) ]\n",
    "    return docs_scores\n",
    "\n",
    "def normalize_faiss_scores(scores):\n",
    "    \"\"\" normalisiert die Scores der Dokumente (zwischen 0 und 1), um Vergleichbarkeit herzustellen\"\"\"\n",
    "    mn, mx = min(scores), max(scores)\n",
    "    if mx == mn:\n",
    "        return [1.0 for _ in scores]  # alle gleich gut -> max confidence\n",
    "    sims = [1.0 - (s - mn) / (mx - mn) for s in scores]  # größer = besser\n",
    "    return sims\n",
    "\n",
    "# LLM als \"Judge\" anlegen\n",
    "class EvalSchema(BaseModel):\n",
    "    solution: str = Field(..., description=\"Proposed solution text to user\")\n",
    "    fix_likelihood: float = Field(..., ge=0.0, le=1.0,\n",
    "        description=\"Model's estimate [0..1] that the solution will resolve the issue\")\n",
    "    rationale: str = Field(..., description=\"One or two sentences explaining why\")\n",
    "\n",
    "judge_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are a cautious support QA checker.\n",
    "Use ONLY the CONTEXT to craft the solution. If the manuals don't contain the answer,\n",
    "set solution to 'NO IMMEDIATE SOLUTION POSSIBLE' and fix_likelihood to 0.0.\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "Return JSON with keys: solution, fix_likelihood, rationale.\n",
    "Keep rationale concise (≤2 sentences).\"\"\"\n",
    ")\n",
    "\n",
    "judge_parser = JsonOutputParser(pydantic_object=EvalSchema)\n",
    "\n",
    "# Verwendung des bestehenden LLMs für die Query\n",
    "judge_chain = judge_prompt | llm | judge_parser \n",
    "\n",
    "def ask_with_confidence(question: str):\n",
    "    # Vektordatenbank abfragen (inklusive Scores)\n",
    "    docs_scores = retrieve_from_vector_db_with_scores(question, k=3)\n",
    "    docs = [d for d, _ in docs_scores]\n",
    "    scores = [s for _, s in docs_scores]\n",
    "    sims = normalize_faiss_scores(scores)           # Normalisierung auf [0..1]\n",
    "    retrieval_conf = sum(sims) / max(1, len(sims))  # Durchschnitt der Scores\n",
    "\n",
    "    # Kontext bauen\n",
    "    ctx = \"\\n\\n\".join(f\"[{i+1}] {d.page_content.strip()}\" for i, d in enumerate(docs))\n",
    "\n",
    "    # Rufe Judge LLM auf\n",
    "    result = judge_chain.invoke({\"question\": question, \"context\": ctx})\n",
    "    solution = result[\"solution\"]  # Modell liefert strukturierte JSON-Daten!\n",
    "    model_conf = float(result[\"fix_likelihood\"])  # Modell liefert strukturierte JSON-Daten!\n",
    "\n",
    "    # Gewichtete Mischung aus der Vektor-DB-Konfidenz und der des Judge-Modells\n",
    "    w_retrieval, w_model = 0.6, 0.4  # 60% aud Vektor-DB zu 40% aus Judge-Modell\n",
    "    final_conf = max(0.0, min(1.0, w_retrieval * retrieval_conf + w_model * model_conf))\n",
    "\n",
    "    return {\n",
    "        \"answer\": solution,\n",
    "        \"confidence\": round(final_conf, 2),\n",
    "        \"retrieval_conf\": round(retrieval_conf, 2),\n",
    "        \"model_conf\": round(model_conf, 2),\n",
    "        \"rationale\": result[\"rationale\"],\n",
    "        \"sources\": [d.metadata.get(\"source\", \"manual_chunk\") for d in docs]\n",
    "    }\n",
    "\n",
    "# Beispiel:\n",
    "# question = \"My StellarWave A9 screen flickers on DisplayPort. What can I do?\"\n",
    "# question = \"My StellarWave A9 is lying on the carpet. What can I do?\"\n",
    "question = \"How can I mount my TV?\"\n",
    "\n",
    "res = ask_with_confidence(question)\n",
    "print(colored(f\"Question: {question}\", \"cyan\", attrs=[\"bold\"]))\n",
    "print(colored(f\"Confidence (final): {res['confidence']:.2f} (retrieval {res['retrieval_conf']:.2f}, judge model {res['model_conf']:.2f})\", \"green\", attrs=[\"bold\"]))\n",
    "print(colored(f\"Rationale: {res['rationale']}\", \"yellow\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_thl (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
